---
layout: post
title: Distributed Backdoor Attacks Against Federated Learning  
tags: [adversarial, distributed, federated learning, robustness]
authors: Xie, Chulin; Huang, Keli; Chen, Pin-Yu; Li, Bo
---

<!-- abstract -->
> As Artificial Intelligence (AI) and its associated activities of machine learning (ML) and deep learning (DL) become embedded in our daily lives, maintaining the security of these systems and the data they use is paramount. While the security of systems is a well researched area, an area of concern often missed is the integrity and reliability of the data which is being used for the training datasets relied on by ML algorithms. We would like to review one of the key paper that touches upon not only the SOTA learning technique, called Federated Learning, that trains shared model without the need to centrally store the data, but also the problems that come with it, or more specifically, how to attack it. This attack method is called Distributed Backdoor Attack, and it is a kind of backdoor attack that is specialized for Federated Learning due to its distributed nature. Before diving into this paper, we want to introduce the readers to certain key concepts that we think are essential to the paper content, as well as touch base on why we think this topic is essential to understand within the bigger context of machine learning.

## Federated Learning

Federated learning (FL) enables training collaborative machine learning models at scale with up to millions of participants while preserving the privacy of the datasets. Unlike in traditional distributed, large-scale ML systems, in which each participant has to send to the centralized server their local dataset to be trained on, in FL, only the models are communicated. The global model is distributed to a subset of participants, who train the model locally on their own data. These new models are sent back to the data server, and averaged to become a new global model for the next round of training. This simple averaging method is first introduced in McMahan et al. (2017), and is called FederatedAveraging (FedAvg for short). While FL is capable of aggregating information provided by different parties for training a better model, it is extremely vulnerable in the presence of faulty and malicious clients. One bad client can compromise the performance and the convergence of the shared model. Thus, although FedAvg is praised for its simplicity and privacy protection ability, it is *not* a robust algorithm, and is very vulnerable to attacks.

## Backdoor attack and Model poisoning

One of the most powerful attacks against FL is called the poisoning attack, where the intruder injects false training data to corrupt the learning model itself. In other words, in a poisoning attack, the attacker compromises the learning process in a way that the system fails on the inputs chosen by the attacker and further constructs a backdoor through which they can control the output even in the future.

However, in the specific case of FL, data-poisoning is not the only option for the attacker. Bagdasaryan et al. (2018) first introduces how FL is vulnerable to **model poisoning**, which is a new class of poisoning attacks. Normal poisoning attack is targeted towards training-data, yet since devices trained under the FL paradigm have access to the global model at each round of training, they have more influence over the entire training process, but only for a few participants.

**Backdoor attack** is a subtype of data-poisoning. On a FL model, a successful backdoor attack is is one that (1) ensures the accuracy on the federated-learning task remains the same (this is so as to bypass FL defence), and (2) trains the model on an attacker-chosen backdoor subtask. For example, a malicious client could train a predictive-keyboard task to suggest advertisement-related information in its suggestions. Thus, understanding how these attacks work and how to generate defense against them is extremely important!
<!-- insert image depicting normal learning vs poisoning attack -->


In order to achieve the aforementioned objective, a backdoor attack would need to transfer to the global model -- this is why simple data poisoning would *NOT* work for FL, because poisoned data in the local modal does not survive averaging. For good effect, the backdoor should survive in the global model for several rounds. It also shouldn't affect overall accuracy of the global model either to avoid being detected.

<p align="center">
  <img src="/public/images/2021-12-01-DBA/dba-pixel-backdoor.png"/>
</p>

To ensure fair comparison between centralized backdoor attack versus DBA, the paper uses *pixel-pattern backdoor* attacks in the experiments. Examples of pixel-pattern backdoor is shown above, in which a classification image with a couple of modifed pixels is categorized as something different. However, in Bagdasaryan et al., *semantic backdoor* is used more frequently, as it is introduced as the more malicious type of attack. In pixel-pattern backdoor, the attacker still needs to physically change the pixel of the image, whereas in semantic backdoor, the attacker does not need to take any extra action (e.g. categorize all green cars as birds). However, since in DBA the authors' main focus is to compare and contrast between centralized backdoor and distributed backdoor, the specific kind of backdoor attack is not important, as long as it is consistent in both attack methods.

In a *model-poisoning* attack, setting the aggregator at round $t+1$ combines information from local parties (benign and adversarial) in the previous round $t$ and update the shared model $G^{t+1}$. Until now, centralized backdoor attacks have been the main point of focus where each participant embeds the same global trigger pattern during training.

<p align="center">
  <img src="/public/images/2021-12-01-DBA/central.png"/>
</p>

The above image is an overview of the centralized attacker. The attack vector is depicted by 4 colors (orange, green, yellow, blue) and each attacker embeds the whole attack vector within their local training data. Although here we only show a single centralized attacker and one adversarial party, in reality a centralized attack can poison multiple parties with the **same** global trigger. 

Let's say the attacker wants to mislead the trained model to predict label $r$ on any input data that has the global trigger attached. The adversarial objective for the attacker $i$ in round $t$ with local dataset $D$ is:

$$w_i^* = argmax_{w_i} \bigg(\sum_{j\in S^i_{poi}} P[G^{t+1}(R(x_j^i, \phi)) = r] + \sum_{j\in S^i_{clean}} P[G^{t+1}(x^i_j) = y_j^i]\bigg)$$

The poisoned dataset and the clean dataset should be disjoint, and together should compose of the entire training dataset. The function $R$ transforms clean data into backdoor data with the trigger pattern using parameter $\phi$. Attacker can also choose the poison ratio $r$, which is the fraction of backdoor samples added, as well as a scale $\gamma$ to scale up malicious model weights. We can see here that there is a balance between the local model fitting the benign samples and fitting the backdoor samples. Increasing $r$ can make the model overfit to the backdoor samples and risk accuracy in the benign samples. At the same time, increasing $\gamma$ will make the attack survive averaging in the global model, yet also risk being detected. However, if we make $r$ or $\gamma$ too small the adversarial samples won't be detected by the global model defence, at the expense the attack will not survive for multiple rounds and therefore the attack might not succeed. 

<p align="center">
  <img src="/public/images/2021-12-01-DBA/dba.png"/>
</p>

On the other hand, this paper explores a novel threat assessment framework in which such global trigger is decomposed into separate local patterns (i.e. the individual colors) which are embeded into the training set of **different** adversarial parties, respectively, which altogether constitutes a complete global pattern as the backdoor trigger. Overall, the main difference between these 2 approaches is that in the centralized setting the attack vector is embedded in full in all adversarial samples while in the distributed setting the attack vector is parsed to multiple adversarial parties. The adversarial objective for DBA is relatively similar to centralized backdoor attack model, with some slight modification:

$$w_i^* = argmax_{w_i} \bigg(\sum_{j\in S^i_{poi}} P[G^{t+1}(R(x_j^i, \phi_i^*)) = r; \gamma, I] + \sum_{j\in S^i_{clean}} P[G^{t+1}(x^i_j) = y_j^i]\bigg)$$

Here, $M$ depicts $M$ attackers with $M$ local triggers. $\phi^*_i$ simply means that the global used in the centralized model is now geometrically decomposed. Each sample in the DBA model only uses a subset of the global trigger.

## Trigger Factors in Distributed Backdoor Attacks
No matter the dataset being used, there are 3 trigger factors found to be critical:

* **Scale** $\gamma$: This parameter is used to scale up the malicious model weights
* **Poison Ratio** $r$: This controls the fraction of backdoored samples added per training batch. Higher $r$ is preferred but there is a trade-off between clean data accuracy and attack success rate. 
* **Poison Interval** $I$: The round of intervals between two poison steps. $I=0$ means all local triggers are embedded within one round, while $I=1$ means the local triggers are embedded in consecutive rounds. 

One might think that increasing $\gamma$ & $r$ and lowering $I$ will yield a more successful attack. However, bare in mind that on top of achieving high attack success rate on backdoored data samples, the global model needs to behave normally on untampered data samples.  

Regarding the trigger factors for image datasets (e.g. MNIST, CIFAR-10, Tiny-imagenet) we have:
* **Trigger Size** $TS$: The number of pixel columns (i.e. width) 
* **Trigger Gap** $TG$: The distance between the left and right, as well as the top and bottom local trigger, respectively 
* **Trigger Location** $TL$: This is the offset of the triger pattern from the top left pixel.  
<!-- include paper image of trigger size, gap and location?????? -->


## Experiment Setup
<!-- explain the table contained in the paper + the attacker ability and the types of attack AM and AS -->

<table>
  <thead>
    <tr>
      <th>Dataset</th>
      <th>Classes</th>
      <th>Features</th>
      <th>Model used</th>
      <th>Benign $l_r$</th>
      <th>Poison $l_r$</th>
      <th>Poison Ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LOAN</td>
      <td>9</td>
      <td>91</td>
      <td>3 fc</td>
      <td>0.001</td>
      <td>0.0005</td>
      <td>10/64</td>
    </tr>
    <tr>
      <td>MNIST</td>
      <td>10</td>
      <td>784</td>
      <td>2 conv and 2 fc</td>
      <td>0.1</td>
      <td>0.05</td>
      <td>20/64</td>
    </tr>
    <tr>
      <td>CIFAR</td>
      <td>10</td>
      <td>1024</td>
      <td>lightweight ResNet-18</td>
      <td>0.1</td>
      <td>0.05</td>
      <td>5/64</td>
    </tr>
    <tr>
      <td>Tiny-imagenet</td>
      <td>200</td>
      <td>4096</td>
      <td>ResNet-18</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>20/64</td>
    </tr>
  </tbody>
</table>

Regarding the attack analysis, the paper evaluates both multiple-shot attacks (A-M) and single-shot attacks (A-S). Attack A-M means the attackers are selected in multiple rounds and the accumulated malicious updates are necessary for a successful attack. Attack A-S means the attacker only needs one single shot to successfully embed its backdoor trigger. In other words, attack A-M studies how easy the backdoor is successfully injected while attack A-S studies how fast the backdoor effect diminishes. For fair comparison purposes, the total number of pixels modified by DBA is equal or less than the pixels modified by the central attack. 

Another important note is that the attack begins when the main accuracy of the global model converges. According to (Bagdasaryan et al., 2018) it's better to attack later in the training process because when the global model is converging, the updates from benign clients contain less commonly shared patterns but more individual features, which are more likely to be canceled out when aggregating and thus having less impact on the backdoor.

Let's analyze the effectiveness of DBA against centralized backdoor attack:

<p align="center">
  <img src="/public/images/2021-12-01-DBA/dba-effectiveness.png"/>
</p>

One important finding from the paper is that *the attack success rate of global trigger is higher than any local trigger even if the global trigger never actually appears in any local training dataset* (Xie et al). We see clearly that in A-M scenarios attack performance converges faster in DBA, whereas in A-S models performance persists for longer. We think it is fascinating that the authors find this result to be consistent among all the training datasets. Due to the distributed nature of FL, updates from devices tend to be more diverse. It is our hypothesis that DBA mimics the nature of FL better than centralized attack model, thus causing attack success rate to converge faster and persists for longer since the global model adapts better to these stealthy changes. Indeed, we will see below that DBA are much more likely to bypass state-of-the-art (SOTA) FL defences.

## Robustness of Distributed Attack

To understand and assess how powerful an attack is, it needs to be tested against SOTA defensive mechanisms. RFA (Pillutla et al., 2019) and FoolsGold (Fung et al., 2018) are 2 robust FL aggregation algorithms. They are based on distance or similarity metrics that make FL robust to corrupted updates by replacing the weighted arithmetic median aggregation with an approximation (e.g. RFA uses a geometric median approximation).

<!-- explain the papers findings against these 2 defense mechanisms -->

Since attack A-S is easily detected because of the scale factor $\gamma$, the paper only analyzes effectiveness of DBA under A-M setting:

<p align="center">
  <img src="/public/images/2021-12-01-DBA/dba-robustness.png"/>
</p>

From reading Pillutla et al., we understand that RFA defends against corrupted models by modifying the mean averaging model with a geometric-mean-based aggregation oracle. In particular, RFA has a "breakdown point" of 1/2, which means that at least 1/2 of the total points (in total weights) must be modified for RFA to *not* converge. Since A-M introduces only a few attackers poisoning a small part of the model every batch, DBA is able to escape RFA's detection and converges.

FoolsGold, on the other hand, does not make any assumption about the proportion of benign samples. Its key insight is that benign updates can be separated from malicious ones by the *diversity* of their gradient updates. However, with multiple attackers poisoning a small fraction of training data with different local patterns, DBA can also bypass FoolsGold. 

## Analysis of Trigger Factors

In the section below, we will summarize the effects of different trigger factors in DBA found in the paper. All the results introduced here are studied under A-S scenarios.

### Scale

It is intuitive that large scale factor leads to a more influential attack, but it will cause the accuracy of the main model to drop during attacking rounds. Furthermore, since the update parameters can be too large with large scale factor, it is easy to be detected, for example, by the RFA defence mechanism mentioned in the robustness section.

### Trigger Location, Gap, and Size

Choosing trigger location and trigger gap is important in image datasets. Trigger locations in the center of the images can obscure the main part of the image, which is essential for accuracy (e.g. in the case of MNIST). Trigger gap that is too large can make the global model fail to recogize the global trigger.

Larger trigger size gives higher attack success rate, but they are stable once the size becomes large enough.

### Poison Interval

The effects of poison interval is intuitive. Having all local attacks submitted at once (poison interval too short) can inflate the model parameters, thus causing the global model's accuracy to fail and is thus susceptible to detection. Having attacks spread too far apart, on the other hand, can make the earlier triggers be forgotten by the main model. 

### Poison Ratio

As we mention above, it is better to keep poison ratio small and the attacks stealthy during local training. Otherwise, we can risk training a model that overfit to the backdoor training samples but fail in accuracy in the benign ones. 

## Conclusion and Reflection

With the introduction of a distributed backdoor attack method that can defeat two robust SOTA defence mechanisms, DBA is a new thread to FL. It is tuned to match the distributed nature of FL, and is stealthy and persistent. Our review of this paper hopefully sheds light on this new type of attack, as well as inspires new assessments for adversarial robustness of FL.

We think this paper is fascinating as it introduces a different concept to what we were exposed to in 10-605/805 at CMU. Most of the distributed, large scale techniques taught in class were focused on a centralized server distributing data and model among smaller clients/devices. Privacy and adversarial robustness issues were studied in-depth. This paper helps us understand more the industrial challenges that real machine learning in large datasets encounter. We can think of FL as a very powerful tool in today's world that can take advantage of millions of users' devices while maintaining the privacy required by the users. However, without careful comprehension of FL and what it does, the model can easily by taken advantage of and the effect can be disastrous. If backdoor attack succeeds, these backdoor subtask will persist not only in the attackers' devices and in the global model, but also in all the benign devices that are unknowingly part of the training process of FL. 

## Extra references

[1] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. *arXiv preprint arXiv:1807.00459*, 2018.

[2] Clement Fung, Chris JM Yoon, and Ivan Baschastnikh. Mitigating sybils in federated learning poisoning. *arXiv preprint arXiv:1808.04866*, 2018.

[3] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust Aggregation for Federated Learning. *arXiv preprint*, 2019.

-----

