---
layout: post
title: Distributed Backdoor Attacks Against Federated Learning  
tags: [adversarial, distributed, federated learning, robustness]
authors: Xie, Chulin; Huang, Keli; Chen, Pin-Yu; Li, Bo
---

<!-- abstract -->
> As Artificial Intelligence (AI) and its associated activities of machine learning (ML) and deep learning (DL) become embedded in our daily lives, maintaining the security of these systems and the data they use is paramount. While the security of systems is a well researched area, an area of concern often missed is the integrity and reliability of the data which is being used for the training datasets relied on by ML algorithms. 

<!-- insert image depicting normal learning vs poisoning attack -->

Federated learning (FL) enables training collaborative machine learning models at scale with many participants while preserving the privacy of the datasets. While FL is capable of aggregating information provided by different parties for training a better model, it fails in the preence of faulty and malicious clients. Thus, one bad client can compromise the entire performance and the convergence of the shared model. 

One of the most powerful attacks is called the poisoning attack, where the intruder injects false training data to corrupt the learning model itself. In other words, in a poisoning attack, the attacker compromises the learning process in a way that the system fails on the inputs chosen by the attacker and further constructs a backdoor through which they can control the output even in the future. 

In a poisoning attack setting the aggregator at round $t+1$ combines information from local parties (benign and adversarial) in the previous round $t$ and update the shared model $G^{t+1}$. Until now, centralized backdoor attacks have been the main point of focus where each participant embeds the same global trigger pattern during training. The new model is equally accurate on the FL task, yet the attacker controls how the model performs on an attacker-chosen backdoor subtask. 

<p align="center">
  <img src="/public/images/2021-12-01-DBA/central.png"/>
</p>

The above image is an overview of the centralized attacker. The attack vector is depicted by 4 colors (orange, green, yellow, blue) and each attacker embeds the whole attack vector within their local training data. Although here I only show a single centralized attacker and one adversarial party, in reality a centralized attack can poison multiple parties with the **same** global trigger. 

<p align="center">
  <img src="/public/images/2021-12-01-DBA/dba.png"/>
</p>

On the other hand, this paper explores a novel threat assessment framework in which such global trigger is decomposed into separate local patterns (i.e. the individual colors) which are embeded into the training set of **different** adversarial parties, respectively, which altogether constitutes a complete global pattern as the backdoor trigger. Overall, the main difference between these 2 approaches is that in the centralized setting the attack vector is embedded in full in all adversarial samples while in the distributed setting the attack vector is parsed to multiple adversarial parties. 


## Trigger Factors in Distributed Backdoor Attacks
No matter the dataset being used, there are 3 trigger factors found to be critical:

* **Scale** $\gamma$: This parameter is used to scale up the malicious model weights
* **Poison Ratio** $r$: This controls the fraction of backdoored samples added per training batch. Higher $r$ is preferred but there is a trade-off between clean data accuracy and attack success rate. 
* **Poison Interval** $I$: The round of intervals between two poison steps. $I=0$ means all local triggers are embedded within one round, while $I=1$ means the local triggers are embedded in consecutive rounds. 

One might think that increasing $\gamma$ & $r$ and lowering $I$ will yield a more successful attack. However, bare in mind that on top of achieving high attack success rate on backdoored data samples, the global model needs to behave normally on untampered data samples.  

Regarding the trigger factors for image datasets (e.g. MNIST, CIFAR-10, Tiny-imagenet) we have:
* **Trigger Size** $TS$: The number of pixel columns (i.e. width) 
* **Trigger Gap** $TG$: The distance between the left and right, as well as the top and bottom local trigger, respectively 
* **Trigger Location** $TL$: This is the offset of the triger pattern from the top left pixel.  
<!-- include paper image of trigger size, gap and location?????? -->


## Experiment Setup
<!-- explain the table contained in the paper + the attacker ability and the types of attack AM and AS -->

<table>
  <thead>
    <tr>
      <th>Dataset</th>
      <th>Classes</th>
      <th>Features</th>
      <th>Model used</th>
      <th>Benign $l_r$</th>
      <th>Poison $l_r$</th>
      <th>Poison Ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LOAN</td>
      <td>9</td>
      <td>91</td>
      <td>3 fc</td>
      <td>0.001</td>
      <td>0.0005</td>
      <td>10/64</td>
    </tr>
    <tr>
      <td>MNIST</td>
      <td>10</td>
      <td>784</td>
      <td>2 conv and 2 fc</td>
      <td>0.1</td>
      <td>0.05</td>
      <td>20/64</td>
    </tr>
    <tr>
      <td>CIFAR</td>
      <td>10</td>
      <td>1024</td>
      <td>lightweight ResNet-18</td>
      <td>0.1</td>
      <td>0.05</td>
      <td>5/64</td>
    </tr>
    <tr>
      <td>Tiny-imagenet</td>
      <td>200</td>
      <td>4096</td>
      <td>ResNet-18</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>20/64</td>
    </tr>
  </tbody>
</table>

Regarding the attack analysis, the paper evaluates both multiple-shot attacks (A-M) and single-shot attacks (A-S). Attack A-M means the attacker are selected in multiple rounds and the accumulated maliciou supdates are necessary for a successful attack. Attack A-S means that the attacker only needs one single shot to successfully embed its backdoor trigger. Specifically, attack A-M studies how easy the backdoor is successfully injected while attack A-S studies how fast the backdoor effect diminishes. For fair comparison, the total number of pixels modified by DBA is equal or less than the pixels modified by the central attack. 

Another important note is that the attack begins when the main accuracy of the global model converges. According to (Bagdasaryan et al., 2018) it's better to attack later in the training process because when the global model is convergin, the updates from benign clients contain less commonly shared patterns but more individual features, which are more likely to be canceled out when aggregating and thus having less impact on the backdoor. 



## Robustness of Distributed Attack
To understand and assess how powerful an attack is, it needs to be tested against state-of-the-art defensive mechanisms. RFA (Pillutla et al., 2019) and FoolsGold (Fung et al., 2018) are 2 robust FL aggregation algorithms. They are based on distance or similarity metrics that make FL robust to corrupted updates by replacing the weighted arithmetic median aggregatiojn with an approximation (e.g. RFA uses a geometric median approximation).

<!-- explain the papers findings against these 2 defense mechanisms -->

## Analysis of Trigger Factors
### Scale

### Poison Ratio

### Poison Interval

### Effects of Trigger Location

### Effects of Trigger Gap

### Effects of Poison Interval


-----

